"""
YoloV8 Inference Controller
===========================

Controller principal para orquestrar inferÃªncia YoloV8.
"""

import json
import time
from pathlib import Path
from typing import Dict, Any, List

from services.inference_service import YoloV8InferenceService
from services.report_service import ReportService
from utils.file_utils import FileUtils
from utils.performance_utils import PerformanceUtils
from config.settings import config


class InferenceController:
    """Controller principal para inferÃªncia YoloV8."""
    
    def __init__(self, model_path: str = None):
        """
        Inicializa o controller.
        
        Args:
            model_path: Caminho do modelo a usar
        """
        self.inference_service = YoloV8InferenceService(model_path)
        self.report_service = ReportService()
        
    def run_single_image_inference(self, image_path: str, conf: float = 0.5) -> Dict[str, Any]:
        """
        Executa inferÃªncia em uma Ãºnica imagem.
        
        Args:
            image_path: Caminho da imagem
            conf: Threshold de confianÃ§a
            
        Returns:
            Resultado da inferÃªncia
        """
        return self.inference_service.run_inference_on_image(image_path, conf)
    
    def run_folder_inference(self, folder_path: str, conf: float = 0.5, 
                           save_report: bool = True) -> Dict[str, Any]:
        """
        Executa inferÃªncia em uma pasta de imagens.
        
        Args:
            folder_path: Caminho da pasta
            conf: Threshold de confianÃ§a
            save_report: Se deve salvar relatÃ³rio
            
        Returns:
            DicionÃ¡rio com resultados e mÃ©tricas
        """
        # Executa inferÃªncia com mediÃ§Ã£o de tempo
        start_time = time.time()
        results = self.inference_service.run_inference_on_folder(folder_path, conf)
        total_time = time.time() - start_time
        
        # Gera relatÃ³rio resumido
        summary = self.report_service.generate_summary_report(results, self.inference_service.model_path)
        
        # Calcula mÃ©tricas de performance
        performance_metrics = PerformanceUtils.calculate_performance_metrics(results, total_time)
        
        # Salva relatÃ³rio se solicitado
        if save_report and results:
            self._save_detailed_report(results, summary, performance_metrics)
        
        return {
            'results': results,
            'summary': summary,
            'performance': performance_metrics,
            'model_used': self.inference_service.model_path
        }
    
    def run_benchmark(self, test_image_path: str, runs: int = 3) -> Dict[str, Any]:
        """
        Executa benchmark do modelo atual.
        
        Args:
            test_image_path: Caminho da imagem de teste
            runs: NÃºmero de execuÃ§Ãµes para mÃ©dia
            
        Returns:
            Resultados do benchmark
        """
        print(f"ðŸ¤– Benchmark do modelo: {Path(self.inference_service.model_path).name}")
        
        # Mede tempo de carregamento (jÃ¡ foi carregado, mas simula)
        load_time = 0.0  # Modelo jÃ¡ carregado
        
        # Executa mÃºltiplas inferÃªncias
        inference_times = []
        total_detections = 0
        
        for i in range(runs):
            start_time = time.time()
            result = self.inference_service.run_inference_on_image(test_image_path, conf=0.5)
            inference_time = time.time() - start_time
            
            inference_times.append(inference_time)
            total_detections += result.get('detections_count', 0)
        
        avg_inference_time = sum(inference_times) / len(inference_times)
        avg_detections = total_detections / runs
        model_size = FileUtils.get_file_size_mb(self.inference_service.model_path)
        
        benchmark_results = {
            'model_name': Path(self.inference_service.model_path).name,
            'load_time': load_time,
            'avg_inference_time': avg_inference_time,
            'avg_detections': avg_detections,
            'model_size': model_size,
            'runs': runs
        }
        
        # Imprime resultados
        print(f"   â±ï¸  Carregamento: {load_time:.2f}s")
        print(f"   ðŸš€ InferÃªncia mÃ©dia: {avg_inference_time:.2f}s")
        print(f"   ðŸŽ¯ DetecÃ§Ãµes mÃ©dias: {avg_detections:.1f}")
        print(f"   ðŸ’¾ Tamanho: {model_size:.1f}MB")
        
        return benchmark_results
    
    def analyze_image_with_different_thresholds(self, image_path: str, 
                                               thresholds: List[float] = None) -> Dict[str, Any]:
        """
        Analisa uma imagem com diferentes thresholds.
        
        Args:
            image_path: Caminho da imagem
            thresholds: Lista de thresholds para testar
            
        Returns:
            Resultados da anÃ¡lise
        """
        if thresholds is None:
            thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]
        
        print(f"ðŸ“· Analisando: {Path(image_path).name}")
        print(f"ðŸŽ¯ Testando diferentes thresholds de confianÃ§a:")
        
        threshold_results = {}
        
        for threshold in thresholds:
            result = self.inference_service.run_inference_on_image(image_path, conf=threshold)
            detections = result.get('detections', [])
            
            threshold_results[threshold] = {
                'detections_count': len(detections),
                'detections': detections
            }
            
            print(f"   Conf {threshold:.1f}: {len(detections)} detecÃ§Ãµes")
            
            # Mostra as 3 melhores detecÃ§Ãµes
            if detections:
                top_detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)[:3]
                for det in top_detections:
                    print(f"      - {det['class_name']}: {det['confidence']:.3f}")
        
        return threshold_results
    
    def _save_detailed_report(self, results: List[Dict[str, Any]], 
                            summary: Dict[str, Any], 
                            performance: Dict[str, Any]):
        """Salva relatÃ³rio detalhado em arquivo JSON."""
        report_data = {
            'summary': summary,
            'performance': performance,
            'detailed_results': results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        output_file = config.OUTPUT_DIR / "inference_report.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"ðŸ’¾ RelatÃ³rio salvo em: {output_file}")
